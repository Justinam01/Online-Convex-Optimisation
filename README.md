# Online Convex Optimization â€“ ImplÃ©mentation de lâ€™algorithme OGD

Ce notebook prÃ©sente une implÃ©mentation pÃ©dagogique de l'algorithme **Online Gradient Descent (OGD)** dans le cadre de lâ€™optimisation convexe en ligne.

## ğŸ“˜ Contexte
L'optimisation convexe en ligne est une mÃ©thode utilisÃ©e dans les systÃ¨mes d'apprentissage adaptatifs, oÃ¹ une sÃ©quence de fonctions convexes est rÃ©vÃ©lÃ©e au fil du temps, et l'objectif est de minimiser le **regret** par rapport Ã  la meilleure dÃ©cision fixe a posteriori.

## âš™ï¸ Contenu
- ImplÃ©mentation de lâ€™algorithme OGD
- Simulation sur une fonction quadratique dynamique
- Ã‰volution du regret cumulÃ© et comparaison avec la borne thÃ©orique
- Visualisation des trajectoires $x_t$ et $theta_t$
- Analyse log-log du regret

## ğŸ› ï¸ Outils
- Python (NumPy, Matplotlib)
- Jupyter Notebook

## ğŸ“ˆ RÃ©sultat
L'algorithme OGD converge avec un regret \( O(\sqrt{T}) \), confirmÃ© numÃ©riquement via la courbe log-log.

